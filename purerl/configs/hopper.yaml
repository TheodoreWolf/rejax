# TODO: brax ppo hyperparams has 5 * (256, ) layer sizes for value, 4 * (32, ) for
# policy. And swish activation. Wtf?

# 10 million timesteps
ppo:
  env: brax/hopper
  env_kwargs: null
  agent_kwargs:
    activation: tanh
    # hidden_layer_sizes: [256, 256]
  num_envs: 2000
  num_steps: 5
  num_epochs: 16
  num_minibatches: 8
  learning_rate: 0.0003
  max_grad_norm: 10
  total_timesteps: 10_000_000
  eval_freq: 100_000
  gamma: 0.99
  gae_lambda: 0.95
  clip_eps: 0.3
  ent_coef: 0.001
  vf_coef: 0.5
  normalize_observations: true


# 500 million timesteps
# ppo:
#   env: brax/hopper
#   env_kwargs: null
#   agent_kwargs:
#     activation: tanh
#   #   hidden_layer_sizes: [256, 256]
#   num_envs: 2000
#   num_steps: 5
#   num_epochs: 4
#   num_minibatches: 4
#   learning_rate: 0.0003
#   max_grad_norm: 10
#   total_timesteps: 500_000_000
#   eval_freq: 5_000_000
#   gamma: 0.99
#   gae_lambda: 0.95
#   clip_eps: 0.3
#   ent_coef: 0.001
#   vf_coef: 0.5
#   normalize_observations: true


sac:
  env: brax/hopper
  env_kwargs: null
  agent_kwargs:
    activation: tanh
    # hidden_layer_sizes: [256, 256]
  num_envs: 256
  buffer_size: 1_048_576
  fill_buffer: 8192
  batch_size: 512
  learning_rate: 0.0006
  gradient_steps: 64
  total_timesteps: 5_242_880
  eval_freq: 40_960
  gamma: 0.95
  tau: 0.995
  target_entropy_ratio: 0
  normalize_observations: true
