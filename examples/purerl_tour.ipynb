{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnJUHuXiF55v"
      },
      "source": [
        "# üì∏ At tour of PureRL: Vectorizable RL Algorithms in Jax\n",
        "\n",
        "`pureRL` is a library of reinforcement learning algorithms which you can `jax.jit` and `jax.vmap`. In this notebook, I want to show you the its key features:\n",
        "\n",
        "- üèÉ‚Äç‚ôÄÔ∏è [Set up](#setup), [train](#training) and [evaluate](#evaluation) RL agents\n",
        "- üîô Use [custom callbacks](#callbacks) to define how training curves are generated, log data to wandb and more\n",
        "- üí´ [Vmap](#vmapping) the train function to train multiple agents in parallel\n",
        "- üé® [Customize](#customizing) existing algorithms by overwriting their methods\n",
        "<br><br>\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/kerajli/purerl/blob/master/examples/purerl_tour.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a> &nbsp; and make sure you set the runtime to GPU!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXikdfzNGPEc",
        "outputId": "721d1578-352e-4340-89c2-348355531a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \"jax[cuda12]==0.4.24\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -q \"flax>=0.7\"\n",
        "!pip install -q git+https://github.com/keraJLi/pureRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEV1qBTqlAkQ"
      },
      "source": [
        "## üî® Setting up the training configuration <a name=\"setup\"></a>\n",
        "\n",
        "1. Each algorithm comes with a config class that extends `flax.PyTreeNode`.\n",
        "2. `get_algo` is a convinient function to access algorithms easily.\n",
        "3. `config_cls.create` creates an instance of the config class, and populates it with default values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UBeDQ1ODeqVy"
      },
      "outputs": [],
      "source": [
        "from purerl import get_algo\n",
        "\n",
        "env_str = \"CartPole-v1\"  # @param [\"CartPole-v1\", \"Acrobot-v1\", \"Pendulum-v1\"]\n",
        "algo_str = \"ppo\"  # @param [\"ppo\", \"dqn\", \"sac\", \"td3\"]\n",
        "\n",
        "# We make some changes to the default hyperparameters to get better plots.\n",
        "# All algorithms should work reasonably well on CartPole-v1 and Acrobot-1.\n",
        "CONFIGS = {\n",
        "    \"sac\": {\"target_entropy_ratio\": 0.7, \"num_envs\": 10, \"gradient_steps\": 5},\n",
        "    \"ppo\": {\"total_timesteps\": 3e5},\n",
        "    \"dqn\": {},\n",
        "    \"td3\": {\"total_timesteps\": 5e4, \"normalize_observations\": True},\n",
        "}\n",
        "\n",
        "algo, config_cls = get_algo(algo_str)\n",
        "config = config_cls.create(env=env_str, learning_rate=0.001, **CONFIGS.get(algo_str, {}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bBZey3rmDKY"
      },
      "source": [
        "Let's look at the config we have created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzagmtYzTK_q",
        "outputId": "93166680-7001-49c9-aeb5-205ea5f7a2d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['calculate_gae',\n",
              " 'collect_trajectories',\n",
              " 'initialize_train_state',\n",
              " 'make_act',\n",
              " 'make_minibatches',\n",
              " 'train',\n",
              " 'train_iteration',\n",
              " 'update',\n",
              " 'update_actor',\n",
              " 'update_critic']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[fn for fn in dir(algo) if not fn.startswith(\"_\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iw3-FjVjXZQ",
        "outputId": "04c91315-a84a-413f-84a5-459fe6d51e2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'env': <gymnax.environments.classic_control.cartpole.CartPole at 0x78068ef8c250>,\n",
              " 'env_params': EnvParams(max_steps_in_episode=500, gravity=9.8, masscart=1.0, masspole=0.1, total_mass=1.1, length=0.5, polemass_length=0.05, force_mag=10.0, tau=0.02, theta_threshold_radians=0.20943951023931953, x_threshold=2.4),\n",
              " 'actor': DiscretePolicy(\n",
              "     # attributes\n",
              "     action_dim = 2\n",
              "     hidden_layer_sizes = (64, 64)\n",
              "     activation = silu\n",
              " ),\n",
              " 'critic': VNetwork(\n",
              "     # attributes\n",
              "     hidden_layer_sizes = (64, 64)\n",
              "     activation = silu\n",
              " ),\n",
              " 'eval_callback': <function purerl.evaluate.make_evaluate.<locals>._evaluate(config, ts, rng)>,\n",
              " 'learning_rate': 0.001,\n",
              " 'gamma': 0.99,\n",
              " 'gae_lambda': 0.95,\n",
              " 'clip_eps': 0.2,\n",
              " 'vf_coef': 0.5,\n",
              " 'ent_coef': 0.01,\n",
              " 'max_grad_norm': inf,\n",
              " 'total_timesteps': 300000.0,\n",
              " 'eval_freq': 10000,\n",
              " 'num_envs': 100,\n",
              " 'num_steps': 50,\n",
              " 'num_epochs': 5,\n",
              " 'num_minibatches': 10,\n",
              " 'normalize_observations': False,\n",
              " 'skip_initial_evaluation': False}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config.__dict__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM__OfJppXtO"
      },
      "source": [
        "As you can see we have a lot of hyperparemeters and variables we can tune. You are free to modify these after the creation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nsbdC7qpRRw",
        "outputId": "30200529-b523-40b1-85b0-3adc49ba9a71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New gamma: 0.995\n"
          ]
        }
      ],
      "source": [
        "config = config.replace(gamma=0.995)\n",
        "print(f\"New gamma: {config.gamma}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_GAsYeeqgiq"
      },
      "source": [
        "**A few words about configs**\n",
        "\n",
        "1. In pureRL, configs extend `flax.struct.PyTreeNode`. This allows to jit and vmap over individual parameters while keeping others fixed. For example, you are free to vmap over `learning_rate`, but not over `total_timesteps`.\n",
        "\n",
        "2. You are free to replace the config you pass to the training algorithm by any object that has the same (or necessary) attributes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43nIrJYarWxU"
      },
      "source": [
        "## üèÉ‚Äç‚ôÄÔ∏è Training the agent <a name=\"training\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "Y-E3hURWqXwu",
        "outputId": "7f9b817a-efe8-4535-f37d-38cca6d1b11c"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36m_init_backend\u001b[0;34m(platform)\u001b[0m\n\u001b[1;32m    760\u001b[0m   \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializing backend '%s'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m   \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m   \u001b[0;31m# TODO(skye): consider raising more descriptive errors directly from backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36mfactory\u001b[0;34m()\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mxla_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_c_api_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplugin_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m     distribute_options = {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jaxlib/xla_client.py\u001b[0m in \u001b[0;36mmake_c_api_client\u001b[0;34m(plugin_name, options, distributed_client)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_xla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_c_api_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplugin_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mXlaRuntimeError\u001b[0m: FAILED_PRECONDITION: No visible GPU devices.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f4177a96eb64>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Set training seed and jit train function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtrain_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/random.py\u001b[0m in \u001b[0;36mPRNGKey\u001b[0;34m(seed, impl)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfold_in\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m   \"\"\"\n\u001b[0;32m--> 240\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_return_prng_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PRNGKey'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/random.py\u001b[0m in \u001b[0;36m_key\u001b[0;34m(ctor_name, seed, impl_spec)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34mf\"{ctor_name} accepts a scalar seed, but was given an array of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         f\"shape {np.shape(seed)} != (). Use jax.vmap for batching\")\n\u001b[0;32m--> 202\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mprng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m def key(seed: int | ArrayLike, *,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/prng.py\u001b[0m in \u001b[0;36mrandom_seed\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    593\u001b[0m   \u001b[0;31m# use-case of instantiating with Python hashes in X32 mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0mseeds_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0mseeds_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, copy)\u001b[0m\n\u001b[1;32m   2215\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2216\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_extended_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2217\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   2170\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected input type for array: {type(object)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m   out_array: Array = lax_internal._convert_element_type(\n\u001b[0m\u001b[1;32m   2173\u001b[0m       out, dtype, weak_type=weak_type)\n\u001b[1;32m   2174\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_convert_element_type\u001b[0;34m(operand, new_dtype, weak_type)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtype_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m     return convert_element_type_p.bind(operand, new_dtype=new_dtype,\n\u001b[0m\u001b[1;32m    561\u001b[0m                                        weak_type=bool(weak_type))\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    442\u001b[0m     assert (not config.enable_checks.value or\n\u001b[1;32m    443\u001b[0m             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_jit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_thread_local_state_disable_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m       \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m       \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjax_jit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswap_thread_local_state_disable_jit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/xla_bridge.py\u001b[0m in \u001b[0;36mbackends\u001b[0;34m()\u001b[0m\n\u001b[1;32m    693\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0m_default_backend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices. (you may need to uninstall the failing plugin package, or set JAX_PLATFORMS=cpu to skip this backend.)"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import time\n",
        "\n",
        "# Set training seed and jit train function\n",
        "rng = jax.random.PRNGKey(0)\n",
        "train_fn = jax.jit(algo.train)\n",
        "\n",
        "print(\"Starting to train\")\n",
        "\n",
        "# Train!\n",
        "start = time.time()\n",
        "train_state, evaluation = train_fn(config, rng)\n",
        "time_elapsed = time.time() - start\n",
        "\n",
        "sps = config.total_timesteps / time_elapsed\n",
        "print(f\"Finished training in {time_elapsed:g} seconds ({sps:g} steps/second).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RvVXjg_C-3u"
      },
      "source": [
        "Whoa, that was pretty quick! Let's break down what just happened.\n",
        "1. We jit the `train_fn` to allow for fast execution üöÄ\n",
        "2. `train_fn` created an initial `train_state`, which holds information about the current state of the algorithm such as the current environment step, replay buffer contents, network parameters and more.\n",
        "3. The `train_state` was passed to the algorithm, which transforms it over the course of training. The final value of the `train_state` is returned, including the final network parameters.\n",
        "4. Additionally, the returned `evaluation` value is a tuple of episode lengths and episodic returns. We take a look at how to customize this value later.\n",
        "\n",
        "Let's look at how well training worked by plotting the learning curve!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go84R1lksJ1T"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "episode_lengths, episode_returns = evaluation\n",
        "mean_return = episode_returns.mean(axis=1)\n",
        "\n",
        "plt.plot(jax.numpy.linspace(0, config.total_timesteps, len(mean_return)), mean_return)\n",
        "plt.xlabel(\"Environment step\")\n",
        "plt.ylabel(\"Episodic return\")\n",
        "plt.title(f\"Training agent for {env} using {algo_str}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZl-61_VIRfE"
      },
      "source": [
        "We should also take a look at the `train_state`, to see what the algorithm produced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ue2CWqB1uc8"
      },
      "outputs": [],
      "source": [
        "jax.tree_map(lambda x: x.shape, train_state).__dict__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUTzC1qMIpGf"
      },
      "source": [
        "In actor-critic algorithms, the `train_state` has an `actor_ts` as well as `critic_ts` which are instances of a `flax.training.train_state.TrainState`. These provide the network parameters of our final agent. Note that while also being a `flax.struct.PyTreeNode`, our `train_state` is not. Algorithms which use buffers also have a `replay_buffer` state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6lB1sWQZwVB"
      },
      "source": [
        "## ü§ñ Making and evaluating policies <a name=\"evaluation\"></a>\n",
        "\n",
        "As discussed above, algorithms return an agent's policy parameters in its train state. We can extract a policy of the type `Callable[[chex.Array, chex.PRNGKey], chex.Array]` which maps `(obs, rng) -> action` like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c52lPsacIh9E"
      },
      "outputs": [],
      "source": [
        "# Get policy and jit it\n",
        "policy = algo.make_act(config, train_state)\n",
        "policy = jax.jit(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aVqH2iGaBXP"
      },
      "source": [
        "Let's evaluate the policy! For demonstration purposes here is a full rollout like you would do with gym:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZREOjRi8Vu6g"
      },
      "outputs": [],
      "source": [
        "# For demonstration purposes, we do a manual rollout of the policy\n",
        "import gymnax\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "env, params = gymnax.make(env_str)\n",
        "step = jax.jit(env.step)\n",
        "\n",
        "obs, state = env.reset(rng, params)\n",
        "episode_return = 0\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    rng, rng_action, rng_step = jax.random.split(rng, 3)\n",
        "    action = policy(obs, rng_action)\n",
        "    obs, state, reward, done, info = step(rng_step, state, action, params)\n",
        "    episode_return += reward\n",
        "\n",
        "print(f\"Return achieved in one episode of {env_str}: {episode_return}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKYhun9FWSnc"
      },
      "source": [
        "Alternatively, `pureRL` offers fast parallel evaluation of policies under `purerl.evaluate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OKEhpiCS_28"
      },
      "outputs": [],
      "source": [
        "from purerl.evaluate import evaluate\n",
        "\n",
        "num_seeds = 200  # @param {type:\"slider\", min:1, max:500, step:1}\n",
        "max_steps = params.max_steps_in_episode\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Evaluation time!\n",
        "episode_lengths, episode_returns = evaluate(policy, rng, env, params, num_seeds, max_steps)\n",
        "\n",
        "time_elapsed = time.time() - start\n",
        "\n",
        "print(\n",
        "    f\"Evaluated {num_seeds} episodes \"\n",
        "    f\"with a total of {jax.numpy.sum(episode_lengths)} environment steps \"\n",
        "    f\"in {time_elapsed:g} seconds.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWCAdMuDXMYM"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(figsize=(8, 4), ncols=2, sharey=\"row\")\n",
        "\n",
        "axes[0].hist(episode_lengths)\n",
        "axes[0].set(title=\"Episode length\", ylabel=\"Count\")\n",
        "axes[1].hist(episode_returns)\n",
        "axes[1].set(title=\"Episode return\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyVpFfziqK-c"
      },
      "source": [
        "# üò≤ Advanced features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHwKyqNRgJqN"
      },
      "source": [
        "## üí´ Vmapping the train function <a name=\"vmapping\"></a>\n",
        "\n",
        "I want to fit an SAC agent for a discrete environment. However, the best `target_entropy_ratio` parameter varies extremely between environments. We can do the following\n",
        "1. `jax.vmap` across the config to run multiple values for the target entropy ratio in parallel\n",
        "2. `jax.vmap` across the training seed to make sure our best run isn't a fluke\n",
        "\n",
        "This can take a minute!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV3kZnXUgJDp"
      },
      "outputs": [],
      "source": [
        "from purerl import SAC, SACConfig\n",
        "\n",
        "num_seeds = 5  # @param {\"type\": \"slider\", \"min\": 1, \"max\": 10, \"step\": 1}\n",
        "env_str = \"Acrobot-v1\"  # @param [\"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\"]\n",
        "\n",
        "def make_config(ter: float):\n",
        "    return SACConfig.create(\n",
        "        env=env_str,\n",
        "        target_entropy_ratio=ter,\n",
        "        num_envs=10,\n",
        "        gradient_steps=5,\n",
        "        )\n",
        "\n",
        "ters = jax.numpy.arange(0.1, 1, 0.1)\n",
        "sac_config = jax.vmap(make_config)(ters)\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "rngs = jax.random.split(rng, num_seeds)\n",
        "\n",
        "sac_train_fn = jax.jit(SAC.train)\n",
        "sac_train_fn = jax.vmap(sac_train_fn, in_axes=(None, 0))  # Vmap across seeds\n",
        "sac_train_fn = jax.vmap(sac_train_fn, in_axes=(0, None))  # Vmap across configs\n",
        "\n",
        "start = time.time()\n",
        "train_state, evaluation = sac_train_fn(config, rngs)\n",
        "time_elapsed = time.time() - start\n",
        "\n",
        "sps = num_seeds * len(ters) * config.total_timesteps / time_elapsed\n",
        "print(\n",
        "    f\"Trained {num_seeds * len(ters)} agents \"\n",
        "    f\"for {sac_config.total_timesteps} environment steps \"\n",
        "    f\"in {time_elapsed // 60:.0f} minutes and {time_elapsed % 60:g} seconds.\"\n",
        ")\n",
        "print(f\"That's {sps:g} steps / second!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJHFbTqjk_Gc"
      },
      "source": [
        "The resulting evaluation now has shape `(num_values, num_train_seeds, num_evals, num_eval_seeds)`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SbE0m7DHeBuY"
      },
      "outputs": [],
      "source": [
        "_, episode_returns = evaluation\n",
        "print(f\"shape of evaluation: {episode_returns.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP2RCX2TjR99"
      },
      "outputs": [],
      "source": [
        "t = jax.numpy.linspace(0, sac_config.total_timesteps, episode_returns.shape[2])\n",
        "\n",
        "# Plot all with alpha\n",
        "for i, returns_for_val in enumerate(episode_returns):\n",
        "    # take mean across evaluation seeds and transpose for plotting\n",
        "    returns_for_val = returns_for_val.mean(axis=2).T\n",
        "    plt.plot(t, returns_for_val, c=f\"C{i}\", alpha=0.3)\n",
        "\n",
        "# Plot mean across training seeds\n",
        "for i, returns_for_val in enumerate(episode_returns):\n",
        "    returns_for_val = returns_for_val.mean(axis=(0, 2))\n",
        "    plt.plot(t, returns_for_val, c=f\"C{i}\", label=f\"{ters[i]:.1f}\")\n",
        "\n",
        "plt.legend(title=\"target_entropy_ratio\", bbox_to_anchor=(1.05, 0.5), loc=\"center left\")\n",
        "plt.xlabel(\"Environment step\")\n",
        "plt.ylabel(\"Episodic return\")\n",
        "plt.title(f\"Different values for SAC target entropy ratio on {env_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLHSi4AAa3vH"
      },
      "source": [
        "## üîô Using custom callbacks <a name=\"callbacks\"></a>\n",
        "\n",
        "If you were super alert, you might have noticed the `eval_callback` attribute of the training config. This can be any function of your choice! As long as it maps `(config, train_state, rng) -> evaluation: chex.ArrayTree`. Two additional attributes control the evaluation behavior:\n",
        "- `eval_freq`: the evaluating of `eval_callback` is called every `eval_freq` environment steps\n",
        "- `skip_initial_evaluation`: if true, don't evaluate the initialized policy (but start after `eval_freq` steps instead).\n",
        "\n",
        "The return value of `eval_callback` should be a PyTree of Jax arrays, so it can be aggregated over the course of training. To show how exactly this works out in practice, we can try to implement our custom callback which returns a dict with some information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1u_-DxorYWIn"
      },
      "outputs": [],
      "source": [
        "def dict_callback(config, train_state, rng):\n",
        "    policy = algo.make_act(config, train_state)\n",
        "    l, r = evaluate(policy, rng, env, params, num_seeds, max_steps)\n",
        "    return {\n",
        "        \"episode_lengths\": l,\n",
        "        \"episode_returns\": r,\n",
        "        \"global_step\": train_state.global_step,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygu2ln1QdvKa"
      },
      "outputs": [],
      "source": [
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "# Replace eval_callback by new one!\n",
        "config = config.replace(eval_callback=dict_callback)\n",
        "_, evaluations = train_fn(config, rng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lL4Yqz5d-SO"
      },
      "outputs": [],
      "source": [
        "print(f\"total_timesteps: {config.total_timesteps}\")\n",
        "print(f\"eval_freq: {config.eval_freq}\")\n",
        "print(f\"Evaluation: {jax.tree_map(lambda x: x.shape, evaluations)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrXB2ZXLeEo_"
      },
      "source": [
        "As you can see, the entries in the evaluation dict have shape `(total_timesteps / eval_freq + 1, num_seeds)`. The `+1` comes from the fact that we evaluate the inital policy as well per default.\n",
        "\n",
        "Since `train_fn` is the jitted `algo.train`, the `eval_callback` is jitted as well, so it must be pure. However, we can escape this via one of Jax's callbacks. For example, we can print the current performance of the agent over the course of training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPX7Xz6OfJVx"
      },
      "outputs": [],
      "source": [
        "def print_callback(config, train_state, rng):\n",
        "    policy = algo.make_act(config, train_state)\n",
        "    _, r = evaluate(policy, rng, env, params, num_seeds, max_steps)\n",
        "\n",
        "    # Jax callback! This can be impure.\n",
        "    jax.debug.print(\"step: {}, mean return: {}\", train_state.global_step, r.mean())\n",
        "\n",
        "    # Since we now print the result, we don't return it\n",
        "    return ()\n",
        "\n",
        "train_fn(config.replace(eval_callback=print_callback), rng)\n",
        "None  # Supress output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QGyMqyFeEmn"
      },
      "source": [
        "Feel free to use `jax.experimental.io_callback`, `jax.debug.callback` or `jax.pure_callback` for more finegrained control over callbacks. For example, I've included an example of how to log to wandb in the repositories `examples` folder!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcWQNxylsksh"
      },
      "source": [
        "## üé® Customize and extend existing algorithms to your liking <a name=\"customizing\"></a>\n",
        "\n",
        "The algorithms in `pureRL` are stateless classes, meaning all of their methods are class methods. You are free to subclass and overwrite these methods! This can be very helpful when trying to modify or extend existing algorithms.\n",
        "\n",
        "Let us replace the actor loss of PPO to create an instance of [Discovered Policy Optimization](https://arxiv.org/abs/2210.05639) [1].\n",
        "In a nutshell, DPO replaces the surrogate objective of PPO with\n",
        "\n",
        "$$\n",
        "L_\\text{actor} = \\sum_{s, a} r A - f(r, A)\n",
        "$$\n",
        "\n",
        "where $A = A^{\\pi_k}(s, a)$,  $r = \\pi(a|s) / \\pi_k(a|s)$ and\n",
        "\n",
        "$$\n",
        "f(r, A) =\n",
        "\\begin{cases}\n",
        "\\text{ReLU}((r - 1)A - \\alpha \\tanh((r - 1)A/\\alpha)) & \\text{if } A \\geq 0, \\\\\n",
        "\\text{ReLU}(\\log(r)A - \\beta \\tanh(\\log(r)A/\\beta)) & \\text{if } A < 0.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "For more detail, see: <br>Chris Lu, Jakub Grudzien Kuba, Alistair Letcher, Luke Metz, Christian Schroeder de Witt, & Jakob Foerster. (2022). Discovered Policy Optimisation.\n",
        "\n",
        "We can easily replace `PPO.update_actor` by this new loss function, while keeping the rest of the algorithm as is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9VUOVsdtR9a"
      },
      "outputs": [],
      "source": [
        "from purerl import PPO, PPOConfig\n",
        "\n",
        "DPOConfig = PPOConfig\n",
        "\n",
        "class DPO(PPO):\n",
        "    @classmethod\n",
        "    def update_actor(cls, config, ts, batch):\n",
        "        def actor_loss_fn(params):\n",
        "            # config.actor is a flax model on which we call the apply function\n",
        "            log_prob, entropy = config.actor.apply(\n",
        "                params,\n",
        "                batch.trajectories.obs,\n",
        "                batch.trajectories.action,\n",
        "                method=\"log_prob_entropy\",\n",
        "            )\n",
        "            entropy = entropy.mean()\n",
        "\n",
        "            # We now calculate actor loss as in DPO\n",
        "            alpha, beta = 2, 0.6\n",
        "            ratio = jax.numpy.exp(log_prob - batch.trajectories.log_prob)\n",
        "            advantages = (batch.advantages - batch.advantages.mean()) / (\n",
        "                batch.advantages.std() + 1e-8\n",
        "            )  # advantage normalization (optional, but improves performance)\n",
        "\n",
        "            # The two drifts represent the cases for A < 0 (A >= 0) in the\n",
        "            # formula for f\n",
        "            drift1 = jax.nn.relu(\n",
        "                (ratio - 1) * advantages\n",
        "                - alpha * jax.numpy.tanh((ratio - 1) * advantages / alpha),\n",
        "            )\n",
        "            drift2 = jax.nn.relu(\n",
        "                jax.numpy.log(ratio) * advantages\n",
        "                - beta * jax.numpy.tanh(jax.numpy.log(ratio) * advantages / beta),\n",
        "            )\n",
        "            drift = jax.numpy.where(advantages >= 0, drift1, drift2)\n",
        "\n",
        "            # Finally, we calculate the actor loss, including an entropy bonus\n",
        "            pi_loss = -(ratio * advantages - drift).mean()\n",
        "            pi_loss = pi_loss - config.ent_coef * entropy\n",
        "            return pi_loss\n",
        "\n",
        "        grads = jax.grad(actor_loss_fn)(ts.actor_ts.params)\n",
        "        return ts.replace(actor_ts=ts.actor_ts.apply_gradients(grads=grads))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoEXAGqcy6bY"
      },
      "source": [
        "Let's see if DPO can train an agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS-mG-bMy5ii"
      },
      "outputs": [],
      "source": [
        "env_str = \"Pendulum-v1\"  # @param [\"CartPole-v1\", \"Acrobot-v1\", \"Pendulum-v1\"]\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "dpo_config = DPOConfig.create(\n",
        "    env=env_str,\n",
        "    total_timesteps=3e5,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=10,\n",
        ")\n",
        "dpo_train_fn = jax.jit(DPO.train)\n",
        "_, (_, returns) = dpo_train_fn(dpo_config, rng)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNj0vZ1-zQPg"
      },
      "outputs": [],
      "source": [
        "plt.plot(\n",
        "    jax.numpy.linspace(0, dpo_config.total_timesteps, len(returns)),\n",
        "    returns.mean(axis=1),\n",
        ")\n",
        "plt.xlabel(\"Environment step\")\n",
        "plt.ylabel(\"Episodic return\")\n",
        "plt.title(f\"Discovered Policy Optimization on {env_str}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5NYJwmk0f91"
      },
      "source": [
        "Seems like it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wZJtXbn0pd7"
      },
      "source": [
        "ü§ó I hope this tour was instructive, let me know if I can help you using pureRL in any way!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
